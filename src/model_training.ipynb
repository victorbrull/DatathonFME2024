{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**This file is for reference only and cannot be executed directly,\n",
        "as it requires access to a confidential CSV data file that is not included in this repository.**\n",
        "\n",
        "**The data used for training contains proprietary company information and is restricted from public access.\n",
        "For demonstration purposes, you can refer to the model file provided in this repository.**"
      ],
      "metadata": {
        "id": "T4cTWXgZmdma"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL6Wh9xzlwTm"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import os\n",
        "import json\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RERRyX9glwTn"
      },
      "source": [
        "### Configuration and Data Loading\n",
        "\n",
        "Loading the California Housing dataset and splitting it into features and target.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CtKpnk1lwTo"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx7ce8SYlwTo"
      },
      "outputs": [],
      "source": [
        "X = train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxi-posNlwTo"
      },
      "outputs": [],
      "source": [
        "# Clean Data\n",
        "\n",
        "X.columns = [col.replace('.', '_') for col in X.columns]\n",
        "\n",
        "\n",
        "y = X['Listing_Price_ClosePrice']\n",
        "\n",
        "# Columns to drop\n",
        "# Some of them are unnecessary information\n",
        "X.drop(columns=[\n",
        "    \"Characteristics_LotFeatures\",\n",
        "    \"Listing_Dates_CloseDate\",\n",
        "    \"Listing_ListingId\",\n",
        "    \"Listing_Price_ClosePrice\",\n",
        "    \"Location_Address_City\",\n",
        "    \"Location_Address_CountyOrParish\",\n",
        "    \"Location_Address_PostalCode\",\n",
        "    \"Location_Address_PostalCodePlus4\",\n",
        "    \"Location_Address_StateOrProvince\",\n",
        "    \"Location_Address_StreetDirectionPrefix\",\n",
        "    \"Location_Address_StreetDirectionSuffix\",\n",
        "    \"Location_Address_StreetName\",\n",
        "    \"Location_Address_StreetNumber\",\n",
        "    \"Location_Address_StreetSuffix\",\n",
        "    \"Location_Address_UnitNumber\",\n",
        "    \"Location_Address_UnparsedAddress\",\n",
        "    \"Location_Area_SubdivisionName\",\n",
        "    \"Location_School_HighSchoolDistrict\",\n",
        "    \"Property_PropertyType\",\n",
        "    \"Structure_Basement\",\n",
        "    \"Structure_Cooling\",\n",
        "    \"Structure_Heating\",\n",
        "    \"Structure_NewConstructionYN\",\n",
        "    \"Structure_ParkingFeatures\",\n",
        "    \"Structure_Rooms_RoomsTotal\",\n",
        "    \"Tax_Zoning\",\n",
        "    \"UnitTypes_UnitTypeType\",\n",
        "    \"ImageData_features_reso_results\",\n",
        "    \"ImageData_room_type_reso_results\",\n",
        "    \"ImageData_style_exterior_summary_label\",\n",
        "    \"ImageData_style_stories_summary_label\",\n",
        "    \"Location_Address_CensusBlock\",\n",
        "    \"Location_Address_CensusTract\"\n",
        "], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n033hb1RlwTo"
      },
      "outputs": [],
      "source": [
        "# Defining parameters\n",
        "test_size = 0.2\n",
        "random_state = 42\n",
        "\n",
        "# Splitting data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57f3tR-RlwTo"
      },
      "source": [
        "### Data Preprocessing Pipeline\n",
        "\n",
        "Setting up a preprocessing pipeline with transformations for numerical and categorical features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kJ8qzY9lwTp"
      },
      "outputs": [],
      "source": [
        "# Identifying numerical and categorical features\n",
        "numeric_features = list(X_train.select_dtypes(include=[np.number]).columns)\n",
        "categorical_features = list(X_train.select_dtypes(include=['object']).columns)\n",
        "\n",
        "# Defining transformations for numerical features\n",
        "numeric_transformers = [('scaler', StandardScaler()), ('poly', PolynomialFeatures(degree=2, include_bias=False))]\n",
        "\n",
        "# Defining transformations for categorical features\n",
        "categorical_transformers = [('onehot', OneHotEncoder(handle_unknown='ignore'))]\n",
        "\n",
        "# Building preprocessor with transformations\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline(steps=numeric_transformers), numeric_features),\n",
        "        ('cat', Pipeline(steps=categorical_transformers), categorical_features)\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF6dR_dglwTp"
      },
      "source": [
        "### Model Selection\n",
        "\n",
        "Selecting and configuring the model based on the configuration. Here we choose Random Forest Regressor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZUVeHNOlwTp"
      },
      "outputs": [],
      "source": [
        "# Defining model parameters\n",
        "model_type = 'random_forest'\n",
        "n_estimators = 100\n",
        "max_depth = 10\n",
        "min_samples_split = 2\n",
        "\n",
        "# Choosing and initialize the model\n",
        "if model_type == 'linear_regression':\n",
        "    model = LinearRegression()\n",
        "elif model_type == 'random_forest':\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        random_state=random_state\n",
        "    )\n",
        "else:\n",
        "    raise ValueError(f\"Model '{model_type}' not supported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_5pvm8plwTp"
      },
      "source": [
        "### Complete Pipeline\n",
        "\n",
        "Combining the preprocessor and the model into a complete pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAmciW2qlwTp"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define your preprocessor steps (e.g., imputing and polynomial features)\n",
        "preprocessor = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Handle NaN values\n",
        "    ('polynomial_features', PolynomialFeatures(degree=2))\n",
        "])\n",
        "\n",
        "# Creating the pipeline with preprocessing and model steps\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', model)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llfYSD7alwTp"
      },
      "source": [
        "### Model Training and Logging in MLflow\n",
        "\n",
        "Trainning the model and log parameters, metrics, and the model itself in MLflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xejmOY_ElwTp"
      },
      "outputs": [],
      "source": [
        "# Starting an MLflow run\n",
        "mlflow.start_run()\n",
        "\n",
        "# Logging parameters to MLflow\n",
        "mlflow.log_param('model_type', model_type)\n",
        "mlflow.log_param('n_estimators', n_estimators)\n",
        "mlflow.log_param('max_depth', max_depth)\n",
        "mlflow.log_param('test_size', test_size)\n",
        "mlflow.log_param('min_samples_split', min_samples_split)\n",
        "\n",
        "# Fitting the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Logging the model to MLflow\n",
        "mlflow.sklearn.log_model(pipeline, artifact_path='model')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx9fgPmxlwTp"
      },
      "source": [
        "### Model Evaluation\n",
        "\n",
        "Evaluating the model on the test set, calculate metrics, and log them in MLflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fs9Js95klwTp"
      },
      "outputs": [],
      "source": [
        "# Predicting on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculating metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Logging metrics to MLflow\n",
        "mlflow.log_metric('mse', mse)\n",
        "mlflow.log_metric('mae', mae)\n",
        "mlflow.log_metric('r2_score', r2)\n",
        "\n",
        "# Printing metrics\n",
        "print(f'MSE: {mse}, MAE: {mae}, R2 Score: {r2}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWT0NUk8lwTp"
      },
      "source": [
        "### Feature Importance Logging\n",
        "\n",
        "Logging feature importances (for models that support it) in MLflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JODgHgZclwTq"
      },
      "outputs": [],
      "source": [
        "# Extracting feature importances from the model (if available)\n",
        "model = pipeline.named_steps['regressor']\n",
        "feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
        "\n",
        "# Checking if the model has feature_importances_ or coefficients\n",
        "if hasattr(model, 'feature_importances_'):\n",
        "    importances = model.feature_importances_\n",
        "elif hasattr(model, 'coef_'):\n",
        "    importances = np.abs(model.coef_)\n",
        "else:\n",
        "    importances = None\n",
        "    print(\"The model does not have 'feature_importances_' or 'coef_' attributes\")\n",
        "\n",
        "if importances is not None:\n",
        "    # Creating a DataFrame for feature importances\n",
        "    feat_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    # Saving and logging feature importances as CSV\n",
        "    feat_imp_csv = 'feature_importance.csv'\n",
        "    feat_imp_df.to_csv(feat_imp_csv, index=False)\n",
        "    mlflow.log_artifact(feat_imp_csv)\n",
        "\n",
        "    # Plotting and saving feature importances\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(feat_imp_df['Feature'], feat_imp_df['Importance'])\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.xlabel('Importance')\n",
        "    plt.title('Feature Importances')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Saving and log the plot\n",
        "    feat_imp_png = 'feature_importance.png'\n",
        "    plt.savefig(feat_imp_png)\n",
        "    plt.close()\n",
        "    mlflow.log_artifact(feat_imp_png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_3Ly_FWlwTq"
      },
      "source": [
        "### SHAP Values Logging\n",
        "\n",
        "Calculating and log SHAP values to explain the model's predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rckC8w7QlwTq"
      },
      "outputs": [],
      "source": [
        "# Directory for SHAP images\n",
        "shap_images_dir = \"shap_images\"\n",
        "os.makedirs(shap_images_dir, exist_ok=True)\n",
        "\n",
        "# Using a sample from the test data to speed up SHAP computation\n",
        "X_test_sample = X_test.sample(n=100, random_state=random_state)\n",
        "\n",
        "# Transforming the data with preprocessing\n",
        "X_transformed = pipeline.named_steps['preprocessor'].transform(X_test_sample)\n",
        "X_transformed_df = pd.DataFrame(X_transformed, columns=feature_names)\n",
        "\n",
        "# Selecting SHAP explainer based on model type\n",
        "if model_type == 'random_forest':\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "else:\n",
        "    explainer = shap.LinearExplainer(model, X_transformed)\n",
        "\n",
        "# Calculating SHAP values\n",
        "shap_values = explainer.shap_values(X_transformed)\n",
        "\n",
        "# Ploting SHAP summary and save\n",
        "shap_summary_png = os.path.join(shap_images_dir, 'shap_summary_plot.png')\n",
        "shap.summary_plot(shap_values, features=X_transformed_df, feature_names=feature_names, show=False)\n",
        "plt.savefig(shap_summary_png, bbox_inches='tight')\n",
        "plt.close()\n",
        "mlflow.log_artifact(shap_summary_png, artifact_path='shap_plots')\n",
        "\n",
        "# Ploting SHAP dependence for the most important feature and save\n",
        "top_feature_index = np.argmax(np.abs(shap_values).mean(0))\n",
        "feature_name = feature_names[top_feature_index]\n",
        "shap_dependence_png = os.path.join(shap_images_dir, 'shap_dependence_plot.png')\n",
        "shap.dependence_plot(feature_name, shap_values, X_transformed_df, feature_names=feature_names, show=False)\n",
        "plt.savefig(shap_dependence_png, bbox_inches='tight')\n",
        "plt.close()\n",
        "mlflow.log_artifact(shap_dependence_png, artifact_path='shap_plots')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rud3lxeKlwTq"
      },
      "source": [
        "### End the MLflow Experiment\n",
        "\n",
        "Ending the MLflow run to finalize the logging.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGGdHLMelwTq"
      },
      "outputs": [],
      "source": [
        "# Ending the MLflow run\n",
        "mlflow.end_run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjgOJ_r5lwTq",
        "outputId": "77e9dc88-bd46-4128-85f8-539b2f9be7c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to model.pkl\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "model = pipeline\n",
        "\n",
        "# Define the file path to save the model\n",
        "model_path = \"model.pkl\"\n",
        "\n",
        "# Save the model to a .pkl file\n",
        "with open(model_path, \"wb\") as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "print(f\"Model saved to {model_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}